{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from tkinter import Tk, filedialog\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from pyclustering.cluster.kmedoids import kmedoids #PAM\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n",
    "\n",
    "#from your_wavecluster_library import WaveCluster  # Replace with the actual import\n",
    "import numpy as np\n",
    "import pywt\n",
    "import networkx as nx\n",
    "from scipy.cluster.hierarchy import linkage, fcluster # For assign_labels()\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances # for CURE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from cure import cure  # You may need to install a library that implements CURE algorithm\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "#Cluster Evaluation\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "#RS\n",
    "import random\n",
    "from numpy import genfromtxt\n",
    "import copy\n",
    "import timeit\n",
    "from scipy.spatial import ConvexHull, distance\n",
    "import collections\n",
    "\n",
    "#Feature Selection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "#from sklearn.feature_selection import \n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "\n",
    "#Filter Method\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#mRMR\n",
    "#from skfeature.function.information_theoretical_based import MRMR\n",
    "#from pymrmr import mRMR\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "#S_Dbw\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>imports</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_DataFrame(file_path):\n",
    "    \"\"\"\n",
    "    Read an Excel file and convert it into a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the Excel file.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the data from the Excel file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Excel file into a DataFrame\n",
    "        df = pd.read_excel(file_path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_excel_file():\n",
    "    \"\"\"\n",
    "    Open a file dialog to choose an Excel file.\n",
    "\n",
    "    Returns:\n",
    "    - str: Path to the selected Excel file.\n",
    "    \"\"\"\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select Excel file\",\n",
    "        filetypes=[(\"Excel files\", \"*.xlsx;*.xls\")],\n",
    "    )\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = choose_excel_file()\n",
    "\n",
    "dataframe = Read_DataFrame(file_path)\n",
    "\n",
    "if dataframe is not None:\n",
    "    print(\"DataFrame created successfully.\")\n",
    "    print(dataframe.head())  # Display the first few rows of the DataFrame\n",
    "else:\n",
    "    print(\"Failed to create DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TC_ID_df = dataframe.copy()\n",
    "\n",
    "dataframe = dataframe.drop(columns=['TC_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Preprocessing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataframe):\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame by encoding categorical columns.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (pandas.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: Processed DataFrame with numerical values.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    for column in dataframe.columns:\n",
    "        if dataframe[column].dtype == 'object':\n",
    "            dataframe[column] = le.fit_transform(dataframe[column]).astype('int64')\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_with_mean(dataframe):\n",
    "    \"\"\"\n",
    "    Replace NaN or null values in a DataFrame with the mean of each column.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with NaN values replaced by mean\n",
    "    \"\"\"\n",
    "    return dataframe.fillna(dataframe.mean()).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = preprocess_data(dataframe)\n",
    "dataframe = fill_na_with_mean(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_value_columns(df):\n",
    "    \"\"\"\n",
    "    Remove columns from a DataFrame that have only one unique value across all rows.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "    - Modified DataFrame with single-value columns removed\n",
    "    \"\"\"\n",
    "    # Identify columns with only one unique value\n",
    "    single_value_columns = df.columns[df.nunique() == 1]\n",
    "\n",
    "    # Drop columns with only one unique value\n",
    "    df = df.drop(single_value_columns, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Single_value_df = remove_single_value_columns(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Single_value_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Single_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index and add it as a column\n",
    "df_reset = Non_Single_value_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>PCA</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(dataframe, num_components=dataframe.shape[1]):\n",
    "    \"\"\"\n",
    "    Apply Principal Component Analysis (PCA) to the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (pd.DataFrame): Input DataFrame.\n",
    "    - num_components (int or None): Number of components to keep. If None, keeps all components.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing PCA results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract features (X)\n",
    "    X = dataframe.values\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=num_components)\n",
    "    pca_result = pca.fit_transform(X)\n",
    "\n",
    "    # Get eigenvalues and indices\n",
    "    eigenvalues = pca.explained_variance_\n",
    "    print(type(eigenvalues))\n",
    "    print(eigenvalues)\n",
    "    indices = eigenvalues.argsort()[::-1]\n",
    "\n",
    "    # Order the columns based on eigenvalues\n",
    "    pca_columns = [f'PC{i + 1}' for i in range(num_components)]\n",
    "    ordered_pca_columns = [pca_columns[i] for i in indices]\n",
    "    pca_dataframe = pd.DataFrame(data=pca_result, columns=ordered_pca_columns)\n",
    "\n",
    "    # Sort eigenvalues\n",
    "    sorted_eigenvalues = eigenvalues[indices]\n",
    "\n",
    "    return pca_dataframe, sorted_eigenvalues, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Single_value_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_list = list()\n",
    "feature_weight_list = list()\n",
    "\n",
    "# Fit a range of PCA models\n",
    "\n",
    "for n in range(1, Non_Single_value_df.shape[1] + 1):\n",
    "    \n",
    "    # Create and fit the model\n",
    "    PCAmod = PCA(n_components=n)\n",
    "    PCAmod.fit(Non_Single_value_df)\n",
    "    \n",
    "    # Store the model and variance\n",
    "    pca_list.append(pd.Series({'n':n, 'model':PCAmod,\n",
    "                               'var': PCAmod.explained_variance_ratio_.sum()}))\n",
    "    \n",
    "    # Calculate and store feature importances\n",
    "    abs_feature_values = np.abs(PCAmod.components_).sum(axis=0)\n",
    "    feature_weight_list.append(pd.DataFrame({'n':n, \n",
    "                                             'features': Non_Single_value_df.columns,\n",
    "                                             'values':abs_feature_values/abs_feature_values.sum()}))\n",
    "    \n",
    "pca_df = pd.concat(pca_list, axis=1).T.set_index('n')\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = (pd.concat(feature_weight_list)\n",
    "               .pivot(index='n', columns='features', values='values')) #Sum up all of the n\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "ax = pca_df['var'].plot(kind='bar')\n",
    "\n",
    "ax.set(xlabel='Number of dimensions',\n",
    "       ylabel='Percent explained variance',\n",
    "       title='Explained Variance vs Dimensions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = features_df.plot(kind='bar', figsize=(13,8))\n",
    "ax.legend(loc='upper right')\n",
    "ax.set(xlabel='Number of dimensions',\n",
    "       ylabel='Relative importance',\n",
    "       title='Feature importance vs Dimensions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Single_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Single_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Time.Wc\n",
    "Non_Single_value_df = Non_Single_value_df.drop(columns=['Time.WC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pdf = preprocess_data(Non_Single_value_df)\n",
    "pca_result_df, eigenvalues, pca_model  = apply_pca(Pdf,Pdf.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result_df_3 = pca_result_df.iloc[:,0:3]\n",
    "pca_result_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result_df = pca_result_df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Going Back to Original</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original\n",
    "Non_Single_value_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Chameleon</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(dataframe, centroids, cluster_labels):\n",
    "    distances = np.zeros(len(dataframe))\n",
    "\n",
    "    for i, (label, row) in enumerate(zip(cluster_labels, dataframe.iterrows())):\n",
    "        centroid = centroids[label]\n",
    "        distances[i] = np.linalg.norm(row[1].values - centroid)\n",
    "\n",
    "    return pd.Series(distances, name='Distance to Centroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pca_result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.linalg.norm(np.array(a) - np.array(b))\n",
    "\n",
    "def knn_graph(df, k, verbose=False):\n",
    "    points = [p[1:] for p in df.itertuples()]\n",
    "    g = nx.Graph()\n",
    "    for i in range(0, len(points)):\n",
    "        g.add_node(i)\n",
    "    if verbose:\n",
    "        print(\"Building kNN graph (k = %d)...\" % (k))\n",
    "    iterpoints = tqdm(enumerate(points), total=len(\n",
    "        points)) if verbose else enumerate(points)\n",
    "    for i, p in iterpoints:\n",
    "        distances = list(map(lambda x: euclidean_distance(p, x), points))\n",
    "        closests = np.argsort(distances)[1:k+1]  # second trough kth closest\n",
    "        for c in closests:\n",
    "            # Check if distance is not zero before adding edge\n",
    "            if distances[c] != 0:\n",
    "                g.add_edge(i, c, weight=1.0 / distances[c], similarity=int(\n",
    "                    1.0 / distances[c] * 1e4))\n",
    "        g.nodes[i]['pos'] = p\n",
    "    g.graph['edge_weight_attr'] = 'similarity'\n",
    "    return g\n",
    "\n",
    "\n",
    "\n",
    "def part_graph(graph, k, df=None):\n",
    "    # Randomly partition nodes into k clusters\n",
    "    clusters = random.choices(range(k), k=len(graph.nodes()))\n",
    "    cluster_dict = {node: cluster for node, cluster in zip(graph.nodes(), clusters)}\n",
    "    nx.set_node_attributes(graph, cluster_dict, 'cluster')\n",
    "    \n",
    "    if df is not None:\n",
    "        df['cluster'] = nx.get_node_attributes(graph, 'cluster').values()\n",
    "    \n",
    "    return graph\n",
    "\n",
    "\n",
    "def pre_part_graph(graph, k, df=None, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"Begin clustering...\")\n",
    "        \n",
    "    # Step 1: Create a feature matrix for k-means\n",
    "    # For simplicity, let's use the node degrees as features\n",
    "    node_features = np.array([list(graph.degree(node)) for node in graph.nodes()])\n",
    "    \n",
    "    # Step 2: Apply k-means clustering\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(node_features)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Step 3: Assign cluster labels to the graph nodes\n",
    "    for i, node in enumerate(graph.nodes()):\n",
    "        graph.nodes[node]['cluster'] = labels[i]\n",
    "    \n",
    "    # Update the DataFrame if provided\n",
    "    if df is not None:\n",
    "        df['cluster'] = pd.Series(nx.get_node_attributes(graph, 'cluster')).value\n",
    "    \n",
    "    return graph\n",
    "\n",
    "\n",
    "def get_cluster(graph, clusters):\n",
    "    nodes = [n for n in graph.node if graph.node[n]['cluster'] in clusters]\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def connecting_edges(partitions, graph):\n",
    "    cut_set = []\n",
    "    for a in partitions[0]:\n",
    "        for b in partitions[1]:\n",
    "            if a in graph:\n",
    "                if b in graph[a]:\n",
    "                    cut_set.append((a, b))\n",
    "    return cut_set\n",
    "\n",
    "\n",
    "def min_cut_bisector(graph):\n",
    "    graph = graph.copy()\n",
    "    graph = part_graph(graph, 2)\n",
    "    partitions = get_cluster(graph, [0]), get_cluster(graph, [1])\n",
    "    return connecting_edges(partitions, graph)\n",
    "\n",
    "\n",
    "def get_weights(graph, edges):\n",
    "    return [graph[edge[0]][edge[1]]['weight'] for edge in edges]\n",
    "\n",
    "\n",
    "def bisection_weights(graph, cluster):\n",
    "    cluster = graph.subgraph(cluster)\n",
    "    edges = min_cut_bisector(cluster)\n",
    "    weights = get_weights(cluster, edges)\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_centers(partitioned_graph):\n",
    "    cluster_centers = {}\n",
    "    for cluster_id in set(nx.get_node_attributes(partitioned_graph, 'cluster').values()):\n",
    "        nodes_in_cluster = [node for node, cluster in nx.get_node_attributes(partitioned_graph, 'cluster').items() if cluster == cluster_id]\n",
    "        cluster_points = [partitioned_graph.nodes[node]['pos'] for node in nodes_in_cluster]\n",
    "        cluster_center = np.mean(cluster_points, axis=0)\n",
    "        cluster_centers[cluster_id] = cluster_center\n",
    "    return cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_chameleon = pca_result_df.copy()\n",
    "\n",
    "# Generate a kNN graph from the DataFrame\n",
    "k = 4  # Specify the value of k for kNN graph\n",
    "graph = knn_graph(pca_chameleon, k)\n",
    "\n",
    "# Partition the graph into clusters\n",
    "k_clusters = 7  # Specify the number of clusters\n",
    "partitioned_graph = part_graph(graph, k_clusters, pca_chameleon)\n",
    "\n",
    "cents = get_cluster_centers(partitioned_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cluster assignment for each node\n",
    "cluster_assignment = nx.get_node_attributes(partitioned_graph, 'cluster')\n",
    "\n",
    "# Define colors for each cluster\n",
    "#colors = ['skyblue', 'salmon', 'green', 'yellow', 'black']  # Add more colors if you have more clusters\n",
    "colors = ['skyblue', 'salmon', 'green', 'yellow', 'black', 'orange', 'purple', 'cyan', 'magenta', 'lime']\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "pos = nx.spring_layout(partitioned_graph)  # You can use different layout algorithms\n",
    "for cluster in set(cluster_assignment.values()):\n",
    "    nodes = [node for node, value in cluster_assignment.items() if value == cluster]\n",
    "    nx.draw_networkx_nodes(partitioned_graph, pos, nodelist=nodes, node_color=colors[cluster], node_size=300, label=f'Cluster {cluster}')\n",
    "nx.draw_networkx_edges(partitioned_graph, pos)\n",
    "nx.draw_networkx_labels(partitioned_graph, pos, font_size=10)\n",
    "plt.title('Graph Visualization with Cluster Colors')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pca_chameleon['cluster'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_list = [list(map(float, arr)) for arr in cents.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wcvr(data, labels, centers):\n",
    "    \"\"\"\n",
    "    Calculate the Within-Cluster Variance Ratio (WCVR) for S_Dbw index.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame, input data\n",
    "    - labels: array-like, cluster labels assigned to each data point\n",
    "    - centers: numpy array, cluster centers\n",
    "\n",
    "    Returns:\n",
    "    - wcvr: float, Within-Cluster Variance Ratio\n",
    "    \"\"\"\n",
    "    num_clusters = len(np.unique(labels))\n",
    "    total_wcv = 0\n",
    "\n",
    "    for cluster_label in range(num_clusters):\n",
    "        if cluster_label in labels and cluster_label < len(centers):\n",
    "            cluster_points = data.loc[labels == cluster_label].values\n",
    "\n",
    "            within_cluster_variance = np.mean(np.sum((cluster_points - centers[cluster_label]) ** 2, axis=1))\n",
    "            total_wcv += within_cluster_variance\n",
    "\n",
    "    wcvr = total_wcv / num_clusters\n",
    "\n",
    "    return wcvr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sd_bw_index(data, labels, centers):\n",
    "    \"\"\"\n",
    "    Calculate the S_Dbw index for clustering validation.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame, input data\n",
    "    - labels: array-like, cluster labels assigned to each data point\n",
    "    - centers: numpy array, cluster centers\n",
    "\n",
    "    Returns:\n",
    "    - sd_bw_index: float, S_Dbw index value\n",
    "    \"\"\"\n",
    "    data_array = data.values  # Convert DataFrame to numpy array\n",
    "    try:\n",
    "        silhouette_avg = silhouette_score(data_array, labels)\n",
    "    except ValueError:\n",
    "        print(\"Only 1 cluster -> S_Dbw is not possible\")\n",
    "\n",
    "    db_index = davies_bouldin_score(data_array, labels)\n",
    "    wcvr = calculate_wcvr(data, labels, centers)\n",
    "\n",
    "    sd_bw_index = (db_index + (1 - silhouette_avg) + wcvr) / 3\n",
    "\n",
    "    return sd_bw_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMeans\n",
    "sd_bw_index = calculate_sd_bw_index(pca_result_df, labels, arr_list)\n",
    "print(\"S_Dbw Index:\", sd_bw_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Get Original Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_data(pca_result_df, centroids, pca_model):\n",
    "    \"\"\"\n",
    "    Get the original data from the centroids and the inverted DataFrame from applying PCA.\n",
    "\n",
    "    Parameters:\n",
    "    - pca_result_df (pandas.DataFrame): DataFrame containing PCA results.\n",
    "    - centroids (numpy.ndarray): Array containing the centroids of each cluster.\n",
    "    - pca_model (sklearn.decomposition.PCA): Fitted PCA model.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: DataFrame containing the original data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Invert PCA transformation to get original data\n",
    "    original_data = pca_model.inverse_transform(pca_result_df.values)\n",
    "\n",
    "    # Convert the array back to a DataFrame\n",
    "    original_data_df = pd.DataFrame(data=original_data, columns=pca_result_df.columns)\n",
    "\n",
    "    # Add centroids to the DataFrame\n",
    "    original_centroids = pca_model.inverse_transform(centroids)\n",
    "    centroids_df = pd.DataFrame(data=original_centroids, columns=pca_result_df.columns)\n",
    "    #original_data_with_centroids_df = pd.concat([original_data_df, centroids_df])\n",
    "\n",
    "    return original_data_df, centroids_df\n",
    "\n",
    "#pca_model = pca_model\n",
    "# Example usage\n",
    "original_data_df, original_centroids_df = get_original_data(pca_result_df, arr_list, pca_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_centroids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip 'Cluster Labels' with pca_result_df.iterrows()\n",
    "zipped_results = zip(labels, pca_result_df.iterrows())\n",
    "sum = 0\n",
    "\n",
    "# Display the results\n",
    "for cluster_label, (index, row) in zipped_results:\n",
    "    sum+= 1\n",
    "    print(f'Cluster Label: {cluster_label}, Index: {index}, Row Values: {row.values}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip 'Cluster Labels' with pca_result_df.iterrows()\n",
    "zipped_results = zip(labels, pca_result_df.iterrows())\n",
    "\n",
    "# Collect the results into a list\n",
    "data_list = []\n",
    "for cluster_label, (index, row) in zipped_results:\n",
    "    data_list.append({'Cluster Label': cluster_label, 'Index': index, 'Row Values': row.values})\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "Checking_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Remove duplicate rows based on 'Cluster Label' and 'Index'\n",
    "Checking_df.drop_duplicates(subset=['Cluster Label', 'Index'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DataFrame with Removed Duplicates:\")\n",
    "Checking_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Checking_df['Row Values'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(labels, name='Cluster Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance between data points and their cluster centroids in the PCA space\n",
    "distance_df = calculate_distance(pca_result_df, arr_list, pd.Series(labels, name='Cluster Labels'))\n",
    "\n",
    "# Combine the original DataFrame with the PCA result, cluster labels, and distance\n",
    "Final_result_df = pd.concat([pca_result_df, pd.Series(labels, name='Cluster Labels'), distance_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_distance_indices = Final_result_df.groupby('Cluster Labels')['Distance to Centroid'].idxmin()\n",
    "\n",
    "# Extract the corresponding rows from the DataFrame\n",
    "min_distance_rows = Final_result_df.loc[min_distance_indices]\n",
    "\n",
    "# Reset the index and name the index column as 'TC'\n",
    "min_distance_rows.reset_index(inplace=True)\n",
    "min_distance_rows.rename(columns={'index': 'TC'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd = pd.DataFrame(min_distance_rows)\n",
    "kd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd['TC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows = Non_Single_value_df.iloc[kd['TC']]\n",
    "\n",
    "print(selected_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows_pd = pd.DataFrame(selected_rows.reset_index(drop=True))\n",
    "selected_rows_pd_Explicit_Minimum_to_centroids = pd.DataFrame(selected_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows_pd_Explicit_Minimum_to_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Selected Rows</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows_indices = selected_rows_pd_Explicit_Minimum_to_centroids.iloc[:, 0].tolist()\n",
    "selected_pdfs = Pdf.iloc[selected_rows_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file_path = ''\n",
    "# Ensure the directory exists, create it if necessary\n",
    "output_directory = os.path.dirname(excel_file_path)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Save the Pandas DataFrames as an Excel file with two sheets\n",
    "with pd.ExcelWriter(excel_file_path, engine='xlsxwriter') as writer:\n",
    "    # Save the first DataFrame to the first sheet (Sheet1)\n",
    "    selected_rows_pd.to_excel(writer, sheet_name='Sheet1', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Plotting using First 3 columns in PCA Dataframe, Cluster label for each PCA and Distance to Centroid</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D scatter plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Assign unique colors to clusters\n",
    "colors = plt.cm.get_cmap('viridis', len(Final_result_df['Cluster Labels'].unique()))\n",
    "\n",
    "# Define unique marker styles for each cluster\n",
    "#marker_styles = ['o', 's', 'D', '^', 'v', 'p', '*', 'h']\n",
    "marker_styles = ['o', 's', 'D', '^', 'v', 'p', '*', 'h', 'x', '+']\n",
    "\n",
    "# Scatter plot for each cluster\n",
    "for cluster_label in Final_result_df['Cluster Labels'].unique():\n",
    "    cluster_data = Final_result_df[Final_result_df['Cluster Labels'] == cluster_label]\n",
    "    ax.scatter(cluster_data['PC1'], cluster_data['PC2'], cluster_data['PC3'], label=f'Cluster {cluster_label}', c=[colors(cluster_label)], marker=marker_styles[cluster_label])\n",
    "\n",
    "# Plot centroids\n",
    "for i, (cluster_label, centroid) in enumerate(zip(pd.Series(labels).unique(), arr_list)):\n",
    "    ax.scatter(centroid[0], centroid[1], centroid[2], marker='x', s=200, label=f'Centroid {i}', c=[colors(i)])#[colors(cluster_label)])\n",
    "\n",
    "cluster_numbers = len(Final_result_df['Cluster Labels'].unique())  # Number of clusters\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('PC1', labelpad=20)\n",
    "ax.set_ylabel('PC2', labelpad=20)\n",
    "#ax.set_zlabel('PC3')\n",
    "ax.set_zlabel('PC3', labelpad=20)  # Adjust the labelpad to move the label away from the axis\n",
    "ax.set_title('3D Scatter Plot of Clusters and Centroids')\n",
    "#ax.legend()\n",
    "# Move legend to top left and make it smaller\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(0, 1), prop={'size': 8})\n",
    "\n",
    "# Format the filename with the number of clusters\n",
    "path_to_image = ''\n",
    "plt.savefig(path_to_image)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1D scatter plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Calculate distances of each data point from each centroid\n",
    "distances = []\n",
    "for centroid in np.array(arr_list):\n",
    "    distance = np.linalg.norm(pca_result_df[['PC1']].values - centroid, axis=1)\n",
    "    distances.append(distance)\n",
    "\n",
    "# Assign colors based on the closest centroid\n",
    "colors = np.argmin(distances, axis=0)\n",
    "\n",
    "# Scatter plot with colored data points\n",
    "scatter = ax.scatter(pca_result_df['PC1'], np.zeros_like(pca_result_df['PC1']), c=colors, cmap='viridis')\n",
    "\n",
    "# Plot centroids\n",
    "for centroid in np.array(arr_list):\n",
    "    ax.scatter(centroid[0], 0, marker='x', s=100, color='black')\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_title('1D Scatter Plot of PC1 with Colored Data Points')\n",
    "ax.legend()\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Cluster')\n",
    "\n",
    "# Save the plot as an image\n",
    "Path_to_Image = ''\n",
    "plt.savefig(Path_to_Image)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Save to Excel</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file_path = ''\n",
    "# Ensure the directory exists, create it if necessary\n",
    "output_directory = os.path.dirname(excel_file_path)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Save the Pandas DataFrames as an Excel file with two sheets\n",
    "with pd.ExcelWriter(excel_file_path, engine='xlsxwriter') as writer:\n",
    "    # Save the first DataFrame to the first sheet (Sheet1)\n",
    "    Non_Single_value_df.to_excel(writer, sheet_name='Original_Data', index=False)\n",
    "\n",
    "    # Save the second DataFrame to the second sheet (Sheet2)\n",
    "    original_data_df.to_excel(writer, sheet_name='Original_data_back_from_PCA', index=False, startrow=0)\n",
    "\n",
    "    selected_rows_pd_Explicit_Minimum_to_centroids.to_excel(writer, sheet_name='TCs_With_min_Dist_to_Centroids', index=False, startrow=0)\n",
    "\n",
    "    original_centroids_df.to_excel(writer, sheet_name='Centroids_back_from_PCA', index=False, startrow=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Single_value_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Single_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Original_selected_rows_pd_Explicit_Minimum_to_centroids = selected_rows_pd_Explicit_Minimum_to_centroids.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows_pd_Explicit_Minimum_to_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_centroids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping index values to corresponding column names\n",
    "new_TC_ID_columns_dict = {i: TC_ID_df.at[i, 'TC_ID'] for i in TC_ID_df.index if i in selected_rows_pd_Explicit_Minimum_to_centroids.index}\n",
    "\n",
    "# Print the dictionary\n",
    "new_TC_ID_columns_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trpose = Original_selected_rows_pd_Explicit_Minimum_to_centroids.transpose().copy()\n",
    "Trpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "Trpose = Trpose.rename(columns=new_TC_ID_columns_dict)\n",
    "Trpose"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
