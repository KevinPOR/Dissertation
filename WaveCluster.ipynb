{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from tkinter import Tk, filedialog\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from pyclustering.cluster.kmedoids import kmedoids #PAM\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#from your_wavecluster_library import WaveCluster  # Replace with the actual import\n",
    "import numpy as np\n",
    "import pywt\n",
    "import networkx as nx\n",
    "from scipy.cluster.hierarchy import linkage, fcluster # For assign_labels()\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances # for CURE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from cure import cure  # You may need to install a library that implements CURE algorithm\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "#Cluster Evaluation\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "#RS\n",
    "import random\n",
    "from numpy import genfromtxt\n",
    "import copy\n",
    "import timeit\n",
    "from scipy.spatial import ConvexHull, distance\n",
    "import collections\n",
    "\n",
    "#Feature Selection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "#from sklearn.feature_selection import \n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "\n",
    "#Filter Method\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#mRMR\n",
    "#from skfeature.function.information_theoretical_based import MRMR\n",
    "#from pymrmr import mRMR\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "#S_Dbw\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>imports</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_DataFrame(file_path):\n",
    "    \"\"\"\n",
    "    Read an Excel file and convert it into a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the Excel file.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the data from the Excel file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Excel file into a DataFrame\n",
    "        df = pd.read_excel(file_path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_excel_file():\n",
    "    \"\"\"\n",
    "    Open a file dialog to choose an Excel file.\n",
    "\n",
    "    Returns:\n",
    "    - str: Path to the selected Excel file.\n",
    "    \"\"\"\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select Excel file\",\n",
    "        filetypes=[(\"Excel files\", \"*.xlsx;*.xls\")],\n",
    "    )\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = choose_excel_file()\n",
    "\n",
    "dataframe = Read_DataFrame(file_path)\n",
    "\n",
    "if dataframe is not None:\n",
    "    print(\"DataFrame created successfully.\")\n",
    "    print(dataframe.head())  # Display the first few rows of the DataFrame\n",
    "else:\n",
    "    print(\"Failed to create DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TC_ID_df = dataframe.copy()\n",
    "\n",
    "dataframe = dataframe.drop(columns=['TC_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Preprocessing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataframe):\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame by encoding categorical columns.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (pandas.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: Processed DataFrame with numerical values.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    for column in dataframe.columns:\n",
    "        if dataframe[column].dtype == 'object':\n",
    "            dataframe[column] = le.fit_transform(dataframe[column]).astype('int64')\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_with_mean(dataframe):\n",
    "    \"\"\"\n",
    "    Replace NaN or null values in a DataFrame with the mean of each column.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with NaN values replaced by mean\n",
    "    \"\"\"\n",
    "    return dataframe.fillna(dataframe.mean()).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = preprocess_data(dataframe)\n",
    "dataframe = fill_na_with_mean(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_value_columns(df):\n",
    "    \"\"\"\n",
    "    Remove columns from a DataFrame that have only one unique value across all rows.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "    - Modified DataFrame with single-value columns removed\n",
    "    \"\"\"\n",
    "    # Identify columns with only one unique value\n",
    "    single_value_columns = df.columns[df.nunique() == 1]\n",
    "\n",
    "    # Drop columns with only one unique value\n",
    "    df = df.drop(single_value_columns, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Single_value_df = remove_single_value_columns(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Single_value_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Single_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index and add it as a column\n",
    "df_reset = Non_Single_value_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>PCA</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(dataframe, num_components=dataframe.shape[1]):\n",
    "    \"\"\"\n",
    "    Apply Principal Component Analysis (PCA) to the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (pd.DataFrame): Input DataFrame.\n",
    "    - num_components (int or None): Number of components to keep. If None, keeps all components.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing PCA results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract features (X)\n",
    "    X = dataframe.values\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=num_components)\n",
    "    pca_result = pca.fit_transform(X)\n",
    "\n",
    "    # Get eigenvalues and indices\n",
    "    eigenvalues = pca.explained_variance_\n",
    "    print(type(eigenvalues))\n",
    "    print(eigenvalues)\n",
    "    indices = eigenvalues.argsort()[::-1]\n",
    "\n",
    "    # Order the columns based on eigenvalues\n",
    "    pca_columns = [f'PC{i + 1}' for i in range(num_components)]\n",
    "    ordered_pca_columns = [pca_columns[i] for i in indices]\n",
    "    pca_dataframe = pd.DataFrame(data=pca_result, columns=ordered_pca_columns)\n",
    "\n",
    "    # Sort eigenvalues\n",
    "    sorted_eigenvalues = eigenvalues[indices]\n",
    "\n",
    "    return pca_dataframe, sorted_eigenvalues, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Single_value_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_list = list()\n",
    "feature_weight_list = list()\n",
    "\n",
    "# Fit a range of PCA models\n",
    "\n",
    "for n in range(1, Non_Single_value_df.shape[1] + 1):\n",
    "    \n",
    "    # Create and fit the model\n",
    "    PCAmod = PCA(n_components=n)\n",
    "    PCAmod.fit(Non_Single_value_df)\n",
    "    \n",
    "    # Store the model and variance\n",
    "    pca_list.append(pd.Series({'n':n, 'model':PCAmod,\n",
    "                               'var': PCAmod.explained_variance_ratio_.sum()}))\n",
    "    \n",
    "    # Calculate and store feature importances\n",
    "    abs_feature_values = np.abs(PCAmod.components_).sum(axis=0)\n",
    "    feature_weight_list.append(pd.DataFrame({'n':n, \n",
    "                                             'features': Non_Single_value_df.columns,\n",
    "                                             'values':abs_feature_values/abs_feature_values.sum()}))\n",
    "    \n",
    "pca_df = pd.concat(pca_list, axis=1).T.set_index('n')\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = (pd.concat(feature_weight_list)\n",
    "               .pivot(index='n', columns='features', values='values')) #Sum up all of the n\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "ax = pca_df['var'].plot(kind='bar')\n",
    "\n",
    "ax.set(xlabel='Number of dimensions',\n",
    "       ylabel='Percent explained variance',\n",
    "       title='Explained Variance vs Dimensions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = features_df.plot(kind='bar', figsize=(13,8))\n",
    "ax.legend(loc='upper right')\n",
    "ax.set(xlabel='Number of dimensions',\n",
    "       ylabel='Relative importance',\n",
    "       title='Feature importance vs Dimensions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Single_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Time.Wc\n",
    "Non_Single_value_df = Non_Single_value_df.drop(columns=['Time.WC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pdf = preprocess_data(Non_Single_value_df)\n",
    "pca_result_df, eigenvalues, pca_model  = apply_pca(Pdf,Pdf.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result_df_3 = pca_result_df.iloc[:,0:3]\n",
    "pca_result_df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result_df = pca_result_df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Going Back to Original</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original\n",
    "Non_Single_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Origin = pca_model.inverse_transform(pca_result_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_Origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>WaveCluster</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(dataframe, centroids, cluster_labels):\n",
    "    distances = np.zeros(len(dataframe))\n",
    "\n",
    "    for i, (label, row) in enumerate(zip(cluster_labels, dataframe.iterrows())):\n",
    "        centroid = centroids[label]\n",
    "        distances[i] = np.linalg.norm(row[1].values - centroid)\n",
    "\n",
    "    return pd.Series(distances, name='Distance to Centroid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pca_result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''In the second step, discrete wavelet transform will be applied on the quantized feature space. Applying wavelet transform on the units Mj results in a new feature space and\n",
    "hence new units Tk. Given the set of units Tk, WaveCluster detects the connected components in the transformed feature space. Each connected component is a set of units Tk and is considered\n",
    "as a cluster. Corresponding to each resolution r of wavelet transform, there would be a set of clusters Cr , where usually at the coarser resolutions, number of clusters is less.\n",
    "In the experiments, we applied wavelet transform three times and tried Haar, Daubechies, Cohen-Daubechies-Feauveau ((4,2) and (2,2)) transforms [Vai93, SN96, URB97].\n",
    "Average subbands (feature spaces) give approximations of the original feature space at different scales, which help in finding clusters at different levels of details. For example, as\n",
    "shown in Figure 5, for a 2-dimensional feature space, the subbands LL show the clusters at different scales. We use the algorithm in [Hor88] to find the connected components in\n",
    "the 2-dimensional feature space (image). The same concept can be generalized for higher dimensions. Figure 12 in Section 5, shows the clusters that WaveCluster found at each scale\n",
    "in different colors.'''\n",
    "\n",
    "def apply_wavelet_transform(data, levels):\n",
    "    # Choose a wavelet and apply the wavelet transform\n",
    "    wavelet = 'db1'  # Replace with the desired wavelet\n",
    "    coefficients = pywt.wavedec(data, wavelet, level=levels)\n",
    "    \n",
    "    # Flatten the coefficients to a 1D array for simplicity\n",
    "    flattened_coefficients = np.concatenate([c.flatten() for c in coefficients])\n",
    "    \n",
    "    return flattened_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters(wavelet_transformed_data, threshold):\n",
    "    # This can be done using connected components or any clustering algorithm\n",
    "    # based on the application. Here, we use a simple threshold.\n",
    "    return (wavelet_transformed_data > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels(clusters):\n",
    "    unique_elements = np.unique(clusters)\n",
    "    print(unique_elements)\n",
    "    labels = np.searchsorted(unique_elements, clusters)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lookup_table(labels):\n",
    "    # Create a lookup table for quick access\n",
    "    unique_labels = np.unique(labels)\n",
    "    lookup_table = {label: np.where(labels == label)[0] for label in unique_labels}\n",
    "    return lookup_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_objects_to_clusters(data, lookup_table):\n",
    "    clustered_objects = {}\n",
    "    for label, indices in lookup_table.items():\n",
    "        # Ensure indices are within the valid range\n",
    "        valid_indices = indices[indices < len(data)]\n",
    "        clustered_objects[label] = data[valid_indices]\n",
    "    return clustered_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wave_cluster(data, quantization_factor=0.1, wavelet_levels=3, threshold=0.5, num_clusters=3):\n",
    "    # Step 1: Quantize feature space and assign objects to units\n",
    "    quantized_data = np.floor(data / quantization_factor)\n",
    "\n",
    "    # Step 2: Apply wavelet transform on the feature space\n",
    "    wavelet_transformed_data = apply_wavelet_transform(quantized_data, wavelet_levels)\n",
    "\n",
    "    # Step 3: Find connected components (clusters) in the subbands of the transformed feature space\n",
    "    clusters = find_clusters(wavelet_transformed_data, threshold)\n",
    "\n",
    "    # Step 4: Assign labels to the units\n",
    "    labels = assign_labels(clusters)\n",
    "\n",
    "    # Step 5: Make the lookup table\n",
    "    lookup_table = make_lookup_table(labels)\n",
    "\n",
    "    # Step 6: Map the objects to the clusters\n",
    "    clustered_objects = map_objects_to_clusters(data, lookup_table)\n",
    "\n",
    "    # Step 7: Calculate the center of each cluster\n",
    "    cluster_centers = []\n",
    "    for label, cluster in clustered_objects.items():\n",
    "        if cluster.size > 0:  # Check if the cluster is not empty\n",
    "            center = np.mean(cluster, axis=0)\n",
    "            cluster_centers.append(center)\n",
    "\n",
    "    # Step 7: Extract cluster labels for each row in the DataFrame\n",
    "    wave_cluster_labels = []\n",
    "    for idx, row in enumerate(data):\n",
    "        for label, cluster_objects in clustered_objects.items():\n",
    "            if any(np.array_equal(row, obj) for obj in cluster_objects):\n",
    "                wave_cluster_labels.append(label)\n",
    "                break\n",
    "            \n",
    "\n",
    "    return clustered_objects, wave_cluster_labels, cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the wave_cluster function to the DataFrame\n",
    "result, wave_cluster_labels, cluster_centers = wave_cluster(pca_result_df.values)\n",
    "\n",
    "# Display the clustered objects\n",
    "for label, objects in result.items():\n",
    "    print(f'Cluster {label}:')\n",
    "    for obj in objects:\n",
    "        print(obj)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import numpy as np\n",
    "import random\n",
    "from math import *\n",
    "\n",
    "def scale_01_data(rawData):\n",
    "    # normalize the raw dataset\n",
    "    dim = rawData.shape[1]  \n",
    "    # the rawData has at least 2 raw, 1 for signal 1 for label\n",
    "    minList = [np.amin(rawData[:,x]) for x in range(0, dim)]\n",
    "    maxList = [np.amax(rawData[:,x])+0.001 for x in range(0, dim)] \n",
    "    # add the [0] and [1] because there is a 'row of label', and 0.001 to avoid 1\n",
    "    toZero = rawData - np.array(minList)\n",
    "    normData = toZero / (np.array(maxList) - np.array(minList))\n",
    "    return(normData)\n",
    "\n",
    "def map2ScaleDomain(dataset, scale=128):\n",
    "    # map the dataset into scale domain for wavelet transform\n",
    "    if scale <= 0 or not(isinstance(scale, int)):\n",
    "        raise ValueError('scale must be a positive integer')\n",
    "    dim = dataset.shape[1]\n",
    "    length = dataset.shape[0]\n",
    "    sd_data = {}\n",
    "    for i in range(0, length):\n",
    "        num = 0\n",
    "        for j in reversed(range(0, dim)):     # start from the most weighted dimension\n",
    "            num += (dataset[i,j]//(1/scale))*pow(scale, j)  # let the numbering start from '0'!\n",
    "        num = int(num)\n",
    "        if sd_data.get(num, 'N/A')=='N/A':\n",
    "            sd_data[num] = 1\n",
    "        else:\n",
    "            sd_data[num] += 1\n",
    "    return sd_data\n",
    "\n",
    "def ndWT(data, dim, scale, wave):\n",
    "    # calculate 1 order n dimensional wavelet transform with numbered grids\n",
    "    wavelets = {'db1':[0.707, 0.707], 'bior1.3':[-0.09, 0.09, 0.707, 0.707, 0.09, -0.09], \\\n",
    "                'db2':[-0.13, 0.224, 0.836, 0.483]}\n",
    "    lowFreq = {}\n",
    "    convolutionLen = len(wavelets.get(wave))-1\n",
    "    lineLen = ceil(scale/2) + ceil((convolutionLen-2)/2)\n",
    "    for inDim in range(0, dim):\n",
    "        for key in data.keys():\n",
    "            coordinate = [] # coordinate start from 0\n",
    "            tempkey = key\n",
    "            for i in range(0, dim):\n",
    "                # get the coordinate for a numbered grid\n",
    "                if i <= dim-inDim-1:\n",
    "                    coordinate.append(tempkey//pow(scale, (dim-1-i)))\n",
    "                    tempkey = tempkey%pow(scale, (dim-1-i))\n",
    "                else:\n",
    "                    coordinate.append(tempkey//pow(lineLen, (dim-1-i)))\n",
    "                    tempkey = tempkey%pow(lineLen, (dim-1-i))\n",
    "            coordinate.reverse()\n",
    "            startCoord = ceil((coordinate[inDim]+1)/2)-1    # to calculate ndwt, signal should start from 1, temporarily convert\n",
    "            startNum = 0    # numbered label for next level of data\n",
    "            for i in range(0, dim):\n",
    "                if i <= inDim:\n",
    "                    if i == inDim:\n",
    "                        startNum += startCoord*pow(lineLen, i)\n",
    "                    else:\n",
    "                        startNum += coordinate[i]*pow(lineLen, i)\n",
    "                else:\n",
    "                    startNum += coordinate[i]*pow(scale, i)\n",
    "            wavelet = wavelets.get(wave)   # for convolution\n",
    "            for i in range(0, convolutionLen//2+1):  \n",
    "                if startCoord+i >= lineLen: # coordinate start from 0 \n",
    "                    break\n",
    "                if lowFreq.get(int(startNum+pow(lineLen, inDim)*i), 'N/A') == 'N/A':\n",
    "                    lowFreq[int(startNum+pow(lineLen, inDim)*i)] = \\\n",
    "                            data[key]*wavelet[int((startCoord+1+i)*2-(coordinate[inDim]+1))]\n",
    "                else:\n",
    "                    lowFreq[int(startNum+pow(lineLen, inDim)*i)] += \\\n",
    "                            data[key]*wavelet[int((startCoord+1+i)*2-(coordinate[inDim]+1))]\n",
    "        data = lowFreq\n",
    "        lowFreq = {}\n",
    "    return data\n",
    "\n",
    "class node():\n",
    "    def __init__(self, key=0, value=0):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        self.process = False\n",
    "        self.cluster = None\n",
    "\n",
    "    def around(self, scale=1, dim=1):\n",
    "        aroundNodeKey = []\n",
    "        coordinate = []\n",
    "        for inDim in range(0, dim):\n",
    "            # we can't afford diagonal searching\n",
    "            dimCoord = self.key // pow(scale, inDim)\n",
    "            if dimCoord == 0:\n",
    "                aroundNodeKey.append(self.key + pow(scale, inDim))\n",
    "            elif dimCoord == scale-1:\n",
    "                aroundNodeKey.append(self.key - pow(scale, inDim))\n",
    "            else:\n",
    "                aroundNodeKey.append(self.key + pow(scale, inDim))\n",
    "                aroundNodeKey.append(self.key - pow(scale, inDim))\n",
    "        return aroundNodeKey\n",
    "\n",
    "def bfs(equal_pair, maxQueue):\n",
    "    if equal_pair == []:\n",
    "        return equal_pair\n",
    "    group = {x:[] for x in range(1, maxQueue)}\n",
    "    result = []\n",
    "    for x, y in equal_pair:\n",
    "        group[x].append(y)\n",
    "        group[y].append(x)\n",
    "    for i in range(1, maxQueue):\n",
    "        if i in group:\n",
    "            if group[i] == []:\n",
    "                del group[i]\n",
    "            else:\n",
    "                queue = [i]\n",
    "                for j in queue:\n",
    "                    if j in group:\n",
    "                        queue += group[j]\n",
    "                        del group[j]\n",
    "                record = list(set(queue))\n",
    "                record.sort()\n",
    "                result.append(record)\n",
    "    return result\n",
    "\n",
    "def build_key_cluster(nodes, equal_list, cutMiniCluster):\n",
    "    cluster_key = {}\n",
    "    for point in nodes.values():\n",
    "        flag = 0\n",
    "        for cluster in equal_list:\n",
    "            if point.cluster in cluster:\n",
    "                point.cluster = cluster[0]\n",
    "                if cluster_key.get(cluster[0], 'N/A') == 'N/A':\n",
    "                    cluster_key[cluster[0]] = [point]\n",
    "                    flag = 1\n",
    "                else:\n",
    "                    cluster_key[cluster[0]].append(point)\n",
    "                    flag = 1\n",
    "                break\n",
    "        if flag == 0:\n",
    "            if cluster_key.get(point.cluster, 'N/A') == 'N/A':\n",
    "                cluster_key[point.cluster] = [point]\n",
    "            else:\n",
    "                cluster_key[point.cluster].append(point)\n",
    "    count = 1\n",
    "    result = {}\n",
    "    for cluster in cluster_key.keys():\n",
    "        if len(cluster_key[cluster]) == 1:\n",
    "            if cluster_key[cluster][0].value < cutMiniCluster:\n",
    "                continue\n",
    "        for p in cluster_key[cluster]:\n",
    "            result[p.key] = count\n",
    "        count += 1\n",
    "    return result\n",
    "\n",
    "def clustering(data, scale, dim, cutMiniCluster):\n",
    "    equal_pair = []\n",
    "    cluster_flag = 1\n",
    "    for point in data.values():\n",
    "        point.process = True\n",
    "        for around in point.around(scale, dim):\n",
    "            if not (data.get(around, 'N/A') == 'N/A'):\n",
    "                around = data.get(around)\n",
    "                if around.cluster is not None:\n",
    "                    if point.cluster is None:\n",
    "                        point.cluster = around.cluster\n",
    "                    elif point.cluster != around.cluster:\n",
    "                        mincluster = min(point.cluster, around.cluster)\n",
    "                        maxcluster = max(point.cluster, around.cluster)\n",
    "                        equal_pair += [(mincluster, maxcluster)]\n",
    "        if point.cluster is None:\n",
    "            point.cluster = cluster_flag\n",
    "            cluster_flag += 1\n",
    "\n",
    "    equal_pair = set(equal_pair)\n",
    "    equal_list = bfs(equal_pair, cluster_flag)\n",
    "    result = build_key_cluster(data, equal_list, cutMiniCluster)\n",
    "    return result\n",
    "\n",
    "def thresholding(data, threshold, scale, dim):\n",
    "    nodes = {}\n",
    "    result = {}\n",
    "    startNode = node(0)\n",
    "    avg = 0\n",
    "    for key, value in data.items():\n",
    "        if value >= threshold:\n",
    "            nodes[key] = node(key, value)\n",
    "            avg += value\n",
    "            if value > startNode.value:\n",
    "                startNode = node(key, value)\n",
    "    cutMiniCluster = avg / len(nodes)\n",
    "    clusters = clustering(nodes, scale, dim, cutMiniCluster)\n",
    "    return clusters\n",
    "\n",
    "def findThreshold(data, threshold):\n",
    "    value = list(data.values())\n",
    "    value.sort(reverse=True)\n",
    "    x = [i for i in range(1, len(value) + 1)]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.scatter(x, value)\n",
    "    ax.axhline(y=threshold, xmin=0, xmax=1, color='r')\n",
    "    plt.show()\n",
    "\n",
    "def markData(normData, cluster, scale):\n",
    "    dim = normData.shape[1]\n",
    "    tags = []\n",
    "    for point in range(0, normData.shape[0]):\n",
    "        number = 0\n",
    "        for inDim in range(0, dim):\n",
    "            number += (normData[point, inDim] // (1 / scale)) * pow(scale, inDim)\n",
    "        if cluster.get(int(number), 'N/A') == 'N/A':\n",
    "            tags.append(0)\n",
    "        else:\n",
    "            tags.append(cluster.get(int(number)))\n",
    "    return tags\n",
    "\n",
    "def waveCluster(data, scale=50, wavelet='db2', threshold=0.5, plot=False):\n",
    "    waveletlen = {'db1': 0, 'db2': 1, 'bior1.3': 2}\n",
    "    normData = scale_01_data(data)\n",
    "    dim = normData.shape[1]\n",
    "    dataDic = map2ScaleDomain(normData, scale)\n",
    "    dwtResult = ndWT(dataDic, dim, scale, wavelet)\n",
    "    if plot: findThreshold(dwtResult, threshold)\n",
    "    lineLen = scale // 2 + waveletlen.get(wavelet)\n",
    "    result = thresholding(dwtResult, threshold, lineLen, dim)\n",
    "    tags = markData(normData, result, lineLen)\n",
    "    \n",
    "    # Finding cluster centers\n",
    "    clustered_objects = {}\n",
    "    for key, value in result.items():\n",
    "        if value not in clustered_objects:\n",
    "            clustered_objects[value] = []\n",
    "        clustered_objects[value].append(normData[key])\n",
    "\n",
    "    cluster_centers = [np.mean(cluster, axis=0) for cluster in clustered_objects.values()]\n",
    "    \n",
    "    return clustered_objects, tags, cluster_centers\n",
    "\n",
    "def draw2Darray(x, y, tag):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    color = tag / np.amax(tag)\n",
    "    rgb = plt.get_cmap('jet')(color)\n",
    "    ax.scatter(x, y, color=rgb)\n",
    "    plt.show()\n",
    "\n",
    "def heatmap2D(data, lineLen):\n",
    "    intensity = np.zeros((lineLen, lineLen))\n",
    "    x = list(range(0, lineLen))\n",
    "    y = x\n",
    "    for key in data.keys():\n",
    "        xIn = key % (lineLen) \n",
    "        yIn = key // (lineLen) \n",
    "        intensity[int(xIn), int(yIn)] = data.get(key)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    plt.pcolormesh(x, y, intensity.T)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wcvr(data, labels, centers):\n",
    "    \"\"\"\n",
    "    Calculate the Within-Cluster Variance Ratio (WCVR) for ICPS index.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame, input data\n",
    "    - labels: array-like, cluster labels assigned to each data point\n",
    "    - centers: numpy array, cluster centers\n",
    "\n",
    "    Returns:\n",
    "    - wcvr: float, Within-Cluster Variance Ratio\n",
    "    \"\"\"\n",
    "    num_clusters = len(np.unique(labels))\n",
    "    total_wcv = 0\n",
    "\n",
    "    for cluster_label in range(num_clusters):\n",
    "        if cluster_label in labels and cluster_label < len(centers):\n",
    "            cluster_points = data.loc[labels == cluster_label].values\n",
    "\n",
    "            within_cluster_variance = np.mean(np.sum((cluster_points - centers[cluster_label]) ** 2, axis=1))\n",
    "            total_wcv += within_cluster_variance\n",
    "\n",
    "    wcvr = total_wcv / num_clusters\n",
    "\n",
    "    return wcvr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ICPS_index(data, labels, centers):\n",
    "    \"\"\"\n",
    "    Calculate the ICPS index for clustering validation.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame, input data\n",
    "    - labels: array-like, cluster labels assigned to each data point\n",
    "    - centers: numpy array, cluster centers\n",
    "\n",
    "    Returns:\n",
    "    - ICPS_index: float, ICPS index value\n",
    "    \"\"\"\n",
    "    data_array = data.values  # Convert DataFrame to numpy array\n",
    "    try:\n",
    "        silhouette_avg = silhouette_score(data_array, labels)\n",
    "    except ValueError:\n",
    "        print(\"Only 1 cluster -> ICPS is not possible\")\n",
    "\n",
    "    db_index = davies_bouldin_score(data_array, labels)\n",
    "    wcvr = calculate_wcvr(data, labels, centers)\n",
    "\n",
    "    ICPS_index = (db_index + (1 - silhouette_avg) + wcvr) / 3\n",
    "\n",
    "    return ICPS_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store boolean arrays for each cluster\n",
    "cluster_masks = []\n",
    "\n",
    "# Create boolean arrays for each cluster\n",
    "for label in wave_cluster_labels:\n",
    "    cluster_mask = np.array([l == label for l in wave_cluster_labels])\n",
    "    cluster_masks.append(cluster_mask)\n",
    "\n",
    "# Calculate ICPS Index for each cluster\n",
    "for label, cluster_mask in zip(wave_cluster_labels, cluster_masks):\n",
    "    ICPS_index = calculate_ICPS_index(pca_result_df, cluster_mask, cluster_centers)\n",
    "    print(f\"ICPS Index for Cluster {label}: {ICPS_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_wave_cluster_labels = set(wave_cluster_labels)\n",
    "set_wave_cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays to lists and cast each element to an integer\n",
    "arr_list = [list(map(float, arr)) for arr in cluster_centers]\n",
    "arr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(wave_cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICPS_index = calculate_ICPS_index(pca_result_df, np.array(wave_cluster_labels), arr_list)\n",
    "print(\"ICPS Index:\", ICPS_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Get Original Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_data(pca_result_df, centroids, pca_model):\n",
    "    \"\"\"\n",
    "    Get the original data from the centroids and the inverted DataFrame from applying PCA.\n",
    "\n",
    "    Parameters:\n",
    "    - pca_result_df (pandas.DataFrame): DataFrame containing PCA results.\n",
    "    - centroids (numpy.ndarray): Array containing the centroids of each cluster.\n",
    "    - pca_model (sklearn.decomposition.PCA): Fitted PCA model.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: DataFrame containing the original data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Invert PCA transformation to get original data\n",
    "    original_data = pca_model.inverse_transform(pca_result_df.values)\n",
    "\n",
    "    # Convert the array back to a DataFrame\n",
    "    original_data_df = pd.DataFrame(data=original_data, columns=pca_result_df.columns)\n",
    "\n",
    "    # Add centroids to the DataFrame\n",
    "    original_centroids = pca_model.inverse_transform(centroids)\n",
    "    centroids_df = pd.DataFrame(data=original_centroids, columns=pca_result_df.columns)\n",
    "    #original_data_with_centroids_df = pd.concat([original_data_df, centroids_df])\n",
    "\n",
    "    return original_data_df, centroids_df\n",
    "\n",
    "#pca_model = pca_model\n",
    "# Example usage\n",
    "original_data_df, original_centroids_df = get_original_data(pca_result_df, arr_list, pca_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_centroids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip(wave_cluster_labels, pca_result_df.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wave_cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip 'Cluster Labels' with pca_result_df.iterrows()\n",
    "zipped_results = zip(wave_cluster_labels, pca_result_df.iterrows())\n",
    "sum = 0\n",
    "\n",
    "# Display the results\n",
    "for cluster_label, (index, row) in zipped_results:\n",
    "    sum+= 1\n",
    "    print(f'Cluster Label: {cluster_label}, Index: {index}, Row Values: {row.values}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip 'Cluster Labels' with pca_result_df.iterrows()\n",
    "zipped_results = zip(wave_cluster_labels, pca_result_df.iterrows())\n",
    "\n",
    "# Collect the results into a list\n",
    "data_list = []\n",
    "for cluster_label, (index, row) in zipped_results:\n",
    "    data_list.append({'Cluster Label': cluster_label, 'Index': index, 'Row Values': row.values})\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "Checking_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Remove duplicate rows based on 'Cluster Label' and 'Index'\n",
    "Checking_df.drop_duplicates(subset=['Cluster Label', 'Index'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DataFrame with Removed Duplicates:\")\n",
    "Checking_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Checking_df['Row Values'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to the original DataFrame\n",
    "\n",
    "# Calculate the distance between data points and their cluster centroids in the PCA space\n",
    "distance_df = calculate_distance(pca_result_df, arr_list, pd.Series(wave_cluster_labels, name='Cluster Labels'))\n",
    "\n",
    "# Combine the original DataFrame with the PCA result, cluster labels, and distance\n",
    "Final_result_df = pd.concat([pca_result_df, pd.Series(wave_cluster_labels, name='Cluster Labels'), distance_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_distance_indices = Final_result_df.groupby('Cluster Labels')['Distance to Centroid'].idxmin()\n",
    "\n",
    "# Extract the corresponding rows from the DataFrame\n",
    "min_distance_rows = Final_result_df.loc[min_distance_indices]\n",
    "\n",
    "# Reset the index and name the index column as 'TC'\n",
    "min_distance_rows.reset_index(inplace=True)\n",
    "min_distance_rows.rename(columns={'index': 'TC'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd = pd.DataFrame(min_distance_rows)\n",
    "kd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd['TC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows = Non_Single_value_df.iloc[kd['TC']]\n",
    "\n",
    "print(selected_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows_pd = pd.DataFrame(selected_rows.reset_index(drop=True))\n",
    "selected_rows_pd_Explicit_Minimum_to_centroids = pd.DataFrame(selected_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows_pd_Explicit_Minimum_to_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Selected Rows</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows_indices = selected_rows_pd_Explicit_Minimum_to_centroids.iloc[:, 0].tolist()\n",
    "selected_pdfs = Pdf.iloc[selected_rows_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "excel_file_path = ''\n",
    "# Ensure the directory exists, create it if necessary\n",
    "output_directory = os.path.dirname(excel_file_path)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Save the Pandas DataFrame as an Excel file\n",
    "\n",
    "# Save the Pandas DataFrames as an Excel file with two sheets\n",
    "with pd.ExcelWriter(excel_file_path, engine='xlsxwriter') as writer:\n",
    "    # Save the first DataFrame to the first sheet (Sheet1)\n",
    "    selected_rows_pd.to_excel(writer, sheet_name='Sheet1', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Plotting using First 3 columns in PCA Dataframe, Cluster label for each PCA and Distance to Centroid</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(wave_cluster_labels).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D scatter plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Assign unique colors to clusters\n",
    "colors = plt.cm.get_cmap('viridis', len(Final_result_df['Cluster Labels'].unique()))\n",
    "\n",
    "# Define unique marker styles for each cluster\n",
    "marker_styles = ['o', 's', 'D', '^', 'v', 'p', '*', 'h']\n",
    "\n",
    "# Scatter plot for each cluster\n",
    "for cluster_label in Final_result_df['Cluster Labels'].unique():\n",
    "    cluster_data = Final_result_df[Final_result_df['Cluster Labels'] == cluster_label]\n",
    "    ax.scatter(cluster_data['PC1'], cluster_data['PC2'], cluster_data['PC3'], label=f'Cluster {cluster_label}', c=[colors(cluster_label)], marker=marker_styles[cluster_label])\n",
    "\n",
    "# Plot centroids\n",
    "for i, (cluster_label, centroid) in enumerate(zip(pd.Series(wave_cluster_labels).unique(), np.array(arr_list))):\n",
    "    ax.scatter(centroid[0], centroid[1], centroid[2], marker='x', s=200, label=f'Centroid {cluster_label}', c=[colors(i)])#[colors(cluster_label)])\n",
    "\n",
    "cluster_numbers = len(Final_result_df['Cluster Labels'].unique())  # Number of clusters\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('PC1', labelpad=20)\n",
    "ax.set_ylabel('PC2', labelpad=20)\n",
    "#ax.set_zlabel('PC3')\n",
    "ax.set_zlabel('PC3', labelpad=20)  # Adjust the labelpad to move the label away from the axis\n",
    "ax.set_title('3D Scatter Plot of Clusters and Centroids')\n",
    "#ax.legend()\n",
    "# Move legend to top left and make it smaller\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(0, 1), prop={'size': 8})\n",
    "\n",
    "# Format the filename with the number of clusters\n",
    "path_to_image = ''\n",
    "plt.savefig(path_to_image)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1D scatter plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Calculate distances of each data point from each centroid\n",
    "distances = []\n",
    "for centroid in np.array(arr_list):\n",
    "    distance = np.linalg.norm(pca_result_df[['PC1']].values - centroid, axis=1)\n",
    "    distances.append(distance)\n",
    "\n",
    "# Assign colors based on the closest centroid\n",
    "colors = np.argmin(distances, axis=0)\n",
    "\n",
    "# Scatter plot with colored data points\n",
    "scatter = ax.scatter(pca_result_df['PC1'], np.zeros_like(pca_result_df['PC1']), c=colors, cmap='viridis')\n",
    "\n",
    "# Plot centroids\n",
    "for centroid in np.array(arr_list):\n",
    "    ax.scatter(centroid[0], 0, marker='x', s=100, color='black')\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_title('1D Scatter Plot of PC1 with Colored Data Points')\n",
    "ax.legend()\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Cluster')\n",
    "\n",
    "# Save the plot as an image\n",
    "Path_to_Image = ''\n",
    "plt.savefig(Path_to_Image)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Save to Excel</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "excel_file_path = ''\n",
    "# Ensure the directory exists, create it if necessary\n",
    "output_directory = os.path.dirname(excel_file_path)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Save the Pandas DataFrames as an Excel file with two sheets\n",
    "with pd.ExcelWriter(excel_file_path, engine='xlsxwriter') as writer:\n",
    "    # Save the first DataFrame to the first sheet (Sheet1)\n",
    "    Non_Single_value_df.to_excel(writer, sheet_name='Original_Data', index=False)\n",
    "\n",
    "    # Save the second DataFrame to the second sheet (Sheet2)\n",
    "    original_data_df.to_excel(writer, sheet_name='Original_data_back_from_PCA', index=False, startrow=0)\n",
    "\n",
    "    selected_rows_pd_Explicit_Minimum_to_centroids.to_excel(writer, sheet_name='TCs_With_min_Dist_to_Centroids', index=False, startrow=0)\n",
    "\n",
    "    original_centroids_df.to_excel(writer, sheet_name='Centroids_back_from_PCA', index=False, startrow=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Single_value_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Single_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Original_selected_rows_pd_Explicit_Minimum_to_centroids = selected_rows_pd_Explicit_Minimum_to_centroids.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows_pd_Explicit_Minimum_to_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_centroids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping index values to corresponding column names\n",
    "new_TC_ID_columns_dict = {i: TC_ID_df.at[i, 'TC_ID'] for i in TC_ID_df.index if i in selected_rows_pd_Explicit_Minimum_to_centroids.index}\n",
    "\n",
    "# Print the dictionary\n",
    "new_TC_ID_columns_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trpose = Original_selected_rows_pd_Explicit_Minimum_to_centroids.transpose().copy()\n",
    "Trpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "Trpose = Trpose.rename(columns=new_TC_ID_columns_dict)\n",
    "Trpose"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
