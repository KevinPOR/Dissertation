{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from tkinter import Tk, filedialog\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from pyclustering.cluster.kmedoids import kmedoids #PAM\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#from your_wavecluster_library import WaveCluster  # Replace with the actual import\n",
    "import numpy as np\n",
    "import pywt\n",
    "import networkx as nx\n",
    "from scipy.cluster.hierarchy import linkage, fcluster # For assign_labels()\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances # for CURE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from cure import cure  # You may need to install a library that implements CURE algorithm\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "#Cluster Evaluation\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "#RS\n",
    "import random\n",
    "from numpy import genfromtxt\n",
    "import copy\n",
    "import timeit\n",
    "from scipy.spatial import ConvexHull, distance\n",
    "import collections\n",
    "\n",
    "#Feature Selection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "#from sklearn.feature_selection import \n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "\n",
    "#Filter Method\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#mRMR\n",
    "#from skfeature.function.information_theoretical_based import MRMR\n",
    "#from pymrmr import mRMR\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "#S_Dbw\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_DataFrame(file_path):\n",
    "    \"\"\"\n",
    "    Read an Excel file and convert it into a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the Excel file.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing the data from the Excel file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Excel file into a DataFrame\n",
    "        df = pd.read_excel(file_path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_excel_file():\n",
    "    \"\"\"\n",
    "    Open a file dialog to choose an Excel file.\n",
    "\n",
    "    Returns:\n",
    "    - str: Path to the selected Excel file.\n",
    "    \"\"\"\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select Excel file\",\n",
    "        filetypes=[(\"Excel files\", \"*.xlsx;*.xls\")],\n",
    "    )\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = choose_excel_file()\n",
    "\n",
    "dataframe = Read_DataFrame(file_path)\n",
    "\n",
    "if dataframe is not None:\n",
    "    print(\"DataFrame created successfully.\")\n",
    "    print(dataframe.head())  # Display the first few rows of the DataFrame\n",
    "else:\n",
    "    print(\"Failed to create DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TC_ID_df = dataframe.copy()\n",
    "\n",
    "dataframe = dataframe.drop(columns=['TC_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataframe):\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame by encoding categorical columns.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (pandas.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: Processed DataFrame with numerical values.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    for column in dataframe.columns:\n",
    "        if dataframe[column].dtype == 'object':\n",
    "            dataframe[column] = le.fit_transform(dataframe[column]).astype('int64')\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_with_mean(dataframe):\n",
    "    \"\"\"\n",
    "    Replace NaN or null values in a DataFrame with the mean of each column.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with NaN values replaced by mean\n",
    "    \"\"\"\n",
    "    return dataframe.fillna(dataframe.mean()).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = preprocess_data(dataframe)\n",
    "dataframe = fill_na_with_mean(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_value_columns(df):\n",
    "    \"\"\"\n",
    "    Remove columns from a DataFrame that have only one unique value across all rows.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "    - Modified DataFrame with single-value columns removed\n",
    "    \"\"\"\n",
    "    # Identify columns with only one unique value\n",
    "    single_value_columns = df.columns[df.nunique() == 1]\n",
    "\n",
    "    # Drop columns with only one unique value\n",
    "    df = df.drop(single_value_columns, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = remove_single_value_columns(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.drop(columns=['Time.WC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=3)\n",
    "dataframe = pca.fit_transform(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get eigenvalues and indices\n",
    "eigenvalues = pca.explained_variance_\n",
    "print(type(eigenvalues))\n",
    "print(eigenvalues)\n",
    "indices = eigenvalues.argsort()[::-1]\n",
    "\n",
    "# Order the columns based on eigenvalues\n",
    "pca_columns = [f'PC{i + 1}' for i in range(3)]\n",
    "ordered_pca_columns = [pca_columns[i] for i in indices]\n",
    "dataframe = pd.DataFrame(data=dataframe, columns=ordered_pca_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "summary_stats = dataframe.describe()\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Histograms\n",
    "dataframe.hist(bins=10, figsize=(10, 5))\n",
    "plt.suptitle('Histograms of Signals')\n",
    "plt.show()\n",
    "\n",
    "# Box plots\n",
    "dataframe.plot(kind='box', subplots=True, layout=(1, len(dataframe.columns)), figsize=(10, 5), title='Box Plots of Signals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = dataframe.corr()\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Heatmap of the correlation matrix\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "# Shapiro-Wilk test for normality\n",
    "for column in dataframe.columns:\n",
    "    stat, p = shapiro(dataframe[column])\n",
    "    #print(f'Signal: {column}, Statistics={stat}, p={p}')\n",
    "    if p > 0.05:\n",
    "        print(f'{column} looks Gaussian (fail to reject H0)')\n",
    "    else:\n",
    "        print(f'{column} does not look Gaussian (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# Z-score method\n",
    "z_scores = zscore(dataframe)\n",
    "abs_z_scores = abs(z_scores)\n",
    "outliers = (abs_z_scores > 3).any(axis=1)\n",
    "print(f'Outliers detected: {dataframe[outliers]}')\n",
    "\n",
    "# IQR method\n",
    "Q1 = dataframe.quantile(0.25)\n",
    "Q3 = dataframe.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_iqr = ((dataframe < (Q1 - 1.5 * IQR)) | (dataframe > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "print(f'Outliers detected using IQR: {dataframe[outliers_iqr]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[outliers_iqr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = dataframe.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Fill missing values with the mean (example)\n",
    "df_filled = dataframe.fillna(dataframe.mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
